# ğŸŒ Multilingual Document Ingestion Pipeline for RAG Systems

## ğŸ“‹ **Overview**

This enterprise-grade pipeline automatically ingests multilingual documents into your RAG/search system by:

1. **ğŸ“ Document Upload** â†’ Stores documents in Azure Blob Storage
2. **ğŸ“„ Text Extraction** â†’ Uses Azure Document Intelligence for complex formats
3. **ğŸ” Language Detection** â†’ Automatically detects document language
4. **ğŸŒ Translation** â†’ Translates foreign documents to English using Azure Translator
5. **âœ‚ï¸ Smart Chunking** â†’ Intelligently chunks content with overlap
6. **ğŸ§  Embeddings** â†’ Generates OpenAI embeddings for vector search
7. **ğŸ” Indexing** â†’ Indexes in Azure Cognitive Search with semantic capabilities

## ğŸš€ **Quick Start**

### **1. Setup**

```bash
# Navigate to the pipeline directory
cd multilingual_rag_pipeline

# Install dependencies
pip install -r requirements.txt

# Copy configuration template
cp multilingual_config.json.template multilingual_config.json

# Edit with your Azure credentials
nano multilingual_config.json
```

### **2. Basic Usage**

```python
from multilingual_ingestion_pipeline import MultilingualRAGPipeline

# Initialize pipeline
pipeline = MultilingualRAGPipeline("multilingual_config.json")

# Process documents
result = pipeline.ingest_documents([
    "french_document.pdf",
    "german_report.docx", 
    "spanish_article.txt"
])

print(f"âœ… Processed: {result.processed_files}/{result.total_files}")
print(f"ğŸŒ Translated: {result.translated_files} documents")
print(f"ğŸ“„ Indexed: {result.total_chunks} chunks")
```

### **3. CLI Usage**

```bash
# Process individual files
python multilingual_ingestion_pipeline.py doc1.pdf doc2.docx doc3.txt

# Batch process directory
python multilingual_ingestion_pipeline.py --batch /path/to/documents

# With custom configuration
python multilingual_ingestion_pipeline.py --config my_config.json doc.pdf

# Verbose logging
python multilingual_ingestion_pipeline.py -v documents/
```

## ğŸ¯ **Key Features**

- âœ… **Multi-format Support**: PDF, DOCX, TXT, MD, HTML
- âœ… **Automatic Translation**: 90+ languages â†’ English
- âœ… **Smart Text Extraction**: Azure Document Intelligence + fallback methods
- âœ… **Enterprise Storage**: Azure Blob Storage with organized structure
- âœ… **Vector Search Ready**: OpenAI embeddings + Azure Cognitive Search
- âœ… **Concurrent Processing**: Parallel document processing
- âœ… **Comprehensive Logging**: Detailed progress and error tracking
- âœ… **Flexible Configuration**: JSON-based settings management

## âš™ï¸ **Required Azure Services**

You'll need to set up the following Azure services:

1. **Azure Translator** - For language detection and translation
2. **Azure Document Intelligence** - For advanced text extraction
3. **Azure Storage Account** - For document and content storage
4. **Azure Cognitive Search** - For search indexing with vector support
5. **Azure OpenAI** - For embedding generation

## ğŸ“Š **Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Documents  â”‚â”€â”€â”€â–¶â”‚   Pipeline   â”‚â”€â”€â”€â–¶â”‚   RAG System    â”‚
â”‚ (Any Lang.) â”‚    â”‚              â”‚    â”‚  (All English)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚            â”‚            â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
         â”‚ Azure  â”‚   â”‚ Azure   â”‚  â”‚  Azure   â”‚
         â”‚ Blob   â”‚   â”‚Translatorâ”‚  â”‚ Search   â”‚
         â”‚Storage â”‚   â”‚   API   â”‚  â”‚  Index   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—ï¸ **Pipeline Components**

- **ğŸ—‚ï¸ MultilingualRAGConfig**: Secure credential management
- **ğŸ“„ DocumentProcessor**: Multi-format text extraction  
- **ğŸŒ TranslationService**: Language detection & translation
- **â˜ï¸ BlobStorageManager**: Azure storage operations
- **ğŸ” SearchIndexManager**: Azure Cognitive Search indexing
- **ğŸ¯ MultilingualRAGPipeline**: Main orchestrator

## ğŸ“ˆ **Configuration**

Fill in your Azure service details in `multilingual_config.json`:

```json
{
  "azure_translator": {
    "endpoint": "https://your-translator.cognitiveservices.azure.com/",
    "subscription_key": "your-key-here",
    "region": "eastus"
  },
  "azure_storage": {
    "account_name": "yourstorageaccount",
    "account_key": "your-storage-key",
    "container_name": "documents"
  },
  "azure_document_intelligence": {
    "endpoint": "https://your-doc-intel.cognitiveservices.azure.com/",
    "key": "your-doc-intel-key"
  },
  "azure_search": {
    "service_name": "your-search-service",
    "admin_key": "your-admin-key",
    "index_name": "multilingual-documents"
  },
  "azure_openai": {
    "endpoint": "https://your-openai.openai.azure.com/",
    "key": "your-openai-key",
    "embedding_deployment": "text-embedding-ada-002"
  }
}
```

## ğŸ”§ **Processing Settings**

Customize processing behavior:

```json
{
  "processing_settings": {
    "chunk_size": 1000,           // Tokens per chunk
    "chunk_overlap": 100,         // Overlap between chunks
    "min_chunk_size": 50,         // Minimum chunk size
    "max_concurrent_files": 5,    // Parallel processing limit
    "translation_enabled": true,  // Enable translation
    "force_translation": false,   // Force translate English docs
    "supported_formats": ["pdf", "docx", "txt", "md", "html"]
  }
}
```

## ğŸ“Š **Processing Workflow**

### **Document Processing Steps**

1. **ğŸ“ File Validation** - Verify file exists and format is supported
2. **â˜ï¸ Blob Storage Upload** - Upload original document with unique URL
3. **ğŸ“„ Text Extraction** - Multi-method text extraction based on format
4. **ğŸ” Language Detection** - Azure Translator language detection
5. **ğŸŒ Translation** - Translate to English if not already English
6. **â˜ï¸ Translated Storage** - Store translated content in organized structure
7. **âœ‚ï¸ Smart Chunking** - Intelligent chunking with configurable overlap
8. **ğŸ§  Embedding Generation** - OpenAI embeddings with retry logic
9. **ğŸ” Search Indexing** - Upload to Azure Cognitive Search with vectors

## ğŸ“ˆ **Results & Statistics**

The pipeline provides comprehensive statistics:

```python
pipeline_result = PipelineResult(
    success=True,
    total_files=10,
    processed_files=9,
    failed_files=1,
    translated_files=7,
    total_chunks=150,
    total_processing_time=120.0
)
```

Individual document results include:
- Success/failure status
- Detected language
- Translation statistics
- Number of chunks created
- Processing time
- Blob URLs for storage references

## ğŸ”Œ **Integration Examples**

### **Flask Web App**

```python
from flask import Flask, request, jsonify
from multilingual_ingestion_pipeline import MultilingualRAGPipeline

app = Flask(__name__)
pipeline = MultilingualRAGPipeline()

@app.route('/upload-documents', methods=['POST'])
def upload_documents():
    files = request.files.getlist('documents')
    
    # Save uploaded files temporarily
    temp_files = []
    for file in files:
        temp_path = f"/tmp/{file.filename}"
        file.save(temp_path)
        temp_files.append(temp_path)
    
    # Process through pipeline
    result = pipeline.ingest_documents(temp_files)
    
    return jsonify({
        'success': result.success,
        'processed_files': result.processed_files,
        'translated_files': result.translated_files,
        'total_chunks': result.total_chunks
    })
```

### **Batch Processing**

```python
import schedule
import time
from pathlib import Path

def process_incoming_documents():
    """Scheduled processing of new documents."""
    watch_directory = Path("/data/incoming")
    pipeline = MultilingualRAGPipeline()
    
    # Find new documents
    supported_formats = pipeline.config.processing_settings['supported_formats']
    new_files = []
    
    for fmt in supported_formats:
        new_files.extend(watch_directory.glob(f"*.{fmt}"))
    
    if new_files:
        print(f"Found {len(new_files)} new documents")
        result = pipeline.ingest_documents([str(f) for f in new_files])

# Schedule processing every 10 minutes
schedule.every(10).minutes.do(process_incoming_documents)
```

## ğŸ› **Troubleshooting**

### **Common Issues**

1. **Configuration Errors**
   - Ensure all Azure service credentials are correct
   - Check service endpoints are accessible
   - Verify services are active in Azure Portal

2. **Document Processing Failures**
   - PDF might be image-based (requires OCR)
   - Check document isn't corrupted
   - Try with different file formats

3. **Translation Rate Limits**
   - Increase `rate_limit_delay` in config
   - Upgrade Azure Translator tier
   - Process fewer files concurrently

### **Debug Mode**

```bash
# Enable verbose logging for debugging
python multilingual_ingestion_pipeline.py -v documents/
```

## ğŸ”’ **Security Best Practices**

1. **Never commit** `multilingual_config.json` with credentials
2. **Use Azure Key Vault** for production deployments
3. **Rotate API keys** regularly
4. **Monitor usage** in Azure Portal

## ğŸ‰ **Success Metrics**

After processing, you'll have:

- âœ… **Unified Knowledge Base**: All documents searchable in English
- âœ… **Semantic Search**: Vector embeddings for intelligent retrieval
- âœ… **Organized Storage**: Documents stored in Azure Blob with clear structure
- âœ… **Translation History**: Complete audit trail of language processing
- âœ… **Scalable Architecture**: Ready for enterprise-scale document volumes

---

**ğŸš€ Ready to transform your multilingual documents into a unified, searchable knowledge base!**

*This pipeline provides enterprise-grade document processing with the flexibility to adapt to your specific requirements while maintaining high performance and reliability.*
